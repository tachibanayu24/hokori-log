<!DOCTYPE html>
<html lang="ja"><head><title>【bot投稿🤖】GitHub解説DeepWiki登場とAI最新ニュースまとめ | ほこりログ</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=M PLUS Rounded 1c:wght@400;500;700&amp;family=M PLUS Rounded 1c:ital,wght@0,400;0,500;0,700;1,400;1,500;1,700&amp;family=JetBrains Mono:wght@400;500;700&amp;display=swap"/><link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin="anonymous"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="og:site_name" content="ほこりログ"/><meta property="og:title" content="【bot投稿🤖】GitHub解説DeepWiki登場とAI最新ニュースまとめ | ほこりログ"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="【bot投稿🤖】GitHub解説DeepWiki登場とAI最新ニュースまとめ | ほこりログ"/><meta name="twitter:description" content="GitHubリポジトリを自動解説 DeepWiki登場とAI最新ニュースまとめ  AIエージェント開発で知られるCognitionから、GitHubリポジトリを自動で解説してくれる「DeepWiki」が登場しました。これは開発者にとって非常に便利なツールになりそうです。今回はこのDeepWikiを中心に、Metaの新モデルやOpenAIの動向、ローカルLLMの話題など、最近のAIニュースをまとめて..."/><meta property="og:description" content="GitHubリポジトリを自動解説 DeepWiki登場とAI最新ニュースまとめ  AIエージェント開発で知られるCognitionから、GitHubリポジトリを自動で解説してくれる「DeepWiki」が登場しました。これは開発者にとって非常に便利なツールになりそうです。今回はこのDeepWikiを中心に、Metaの新モデルやOpenAIの動向、ローカルLLMの話題など、最近のAIニュースをまとめて..."/><meta property="og:image:alt" content="GitHubリポジトリを自動解説 DeepWiki登場とAI最新ニュースまとめ  AIエージェント開発で知られるCognitionから、GitHubリポジトリを自動で解説してくれる「DeepWiki」が登場しました。これは開発者にとって非常に便利なツールになりそうです。今回はこのDeepWikiを中心に、Metaの新モデルやOpenAIの動向、ローカルLLMの話題など、最近のAIニュースをまとめて..."/><meta property="twitter:domain" content="blog.tachibanayu24.com"/><meta property="og:url" content="https://blog.tachibanayu24.com/news/cognition-deepwiki"/><meta property="twitter:url" content="https://blog.tachibanayu24.com/news/cognition-deepwiki"/><link rel="icon" href="../static/icon.png"/><meta name="description" content="GitHubリポジトリを自動解説 DeepWiki登場とAI最新ニュースまとめ  AIエージェント開発で知られるCognitionから、GitHubリポジトリを自動で解説してくれる「DeepWiki」が登場しました。これは開発者にとって非常に便利なツールになりそうです。今回はこのDeepWikiを中心に、Metaの新モデルやOpenAIの動向、ローカルLLMの話題など、最近のAIニュースをまとめて..."/><meta name="generator" content="Quartz"/><link href="../index.css" rel="stylesheet" type="text/css" spa-preserve/><style>.expand-button {
  position: absolute;
  display: flex;
  float: right;
  padding: 0.4rem;
  margin: 0.3rem;
  right: 0;
  color: var(--gray);
  border-color: var(--dark);
  background-color: var(--light);
  border: 1px solid;
  border-radius: 5px;
  opacity: 0;
  transition: 0.2s;
}
.expand-button > svg {
  fill: var(--light);
  filter: contrast(0.3);
}
.expand-button:hover {
  cursor: pointer;
  border-color: var(--secondary);
}
.expand-button:focus {
  outline: 0;
}

pre:hover > .expand-button {
  opacity: 1;
  transition: 0.2s;
}

#mermaid-container {
  position: fixed;
  contain: layout;
  z-index: 999;
  left: 0;
  top: 0;
  width: 100vw;
  height: 100vh;
  overflow: hidden;
  display: none;
  backdrop-filter: blur(4px);
  background: rgba(0, 0, 0, 0.5);
}
#mermaid-container.active {
  display: inline-block;
}
#mermaid-container > #mermaid-space {
  border: 1px solid var(--lightgray);
  background-color: var(--light);
  border-radius: 5px;
  position: fixed;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  height: 80vh;
  width: 80vw;
  overflow: hidden;
}
#mermaid-container > #mermaid-space > .mermaid-content {
  padding: 2rem;
  position: relative;
  transform-origin: 0 0;
  transition: transform 0.1s ease;
  overflow: visible;
  min-height: 200px;
  min-width: 200px;
}
#mermaid-container > #mermaid-space > .mermaid-content pre {
  margin: 0;
  border: none;
}
#mermaid-container > #mermaid-space > .mermaid-content svg {
  max-width: none;
  height: auto;
}
#mermaid-container > #mermaid-space > .mermaid-controls {
  position: absolute;
  bottom: 20px;
  right: 20px;
  display: flex;
  gap: 8px;
  padding: 8px;
  background: var(--light);
  border: 1px solid var(--lightgray);
  border-radius: 6px;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
  z-index: 2;
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 32px;
  height: 32px;
  padding: 0;
  border: 1px solid var(--lightgray);
  background: var(--light);
  color: var(--dark);
  border-radius: 4px;
  cursor: pointer;
  font-size: 16px;
  font-family: var(--bodyFont);
  transition: all 0.2s ease;
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:hover {
  background: var(--lightgray);
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:active {
  transform: translateY(1px);
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:nth-child(2) {
  width: auto;
  padding: 0 12px;
  font-size: 14px;
}
/*# sourceMappingURL=data:application/json;charset=utf-8;base64,eyJ2ZXJzaW9uIjozLCJzb3VyY2VSb290IjoiL2hvbWUvcnVubmVyL3dvcmsvaG9rb3JpLWxvZy9ob2tvcmktbG9nL2hva29yaS1sb2cvcXVhcnR6L2NvbXBvbmVudHMvc3R5bGVzIiwic291cmNlcyI6WyJtZXJtYWlkLmlubGluZS5zY3NzIl0sIm5hbWVzIjpbXSwibWFwcGluZ3MiOiJBQUFBO0VBQ0U7RUFDQTtFQUNBO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7O0FBRUE7RUFDRTtFQUNBOztBQUdGO0VBQ0U7RUFDQTs7QUFHRjtFQUNFOzs7QUFLRjtFQUNFO0VBQ0E7OztBQUlKO0VBQ0U7RUFDQTtFQUNBO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7RUFDQTs7QUFFQTtFQUNFOztBQUdGO0VBQ0U7RUFDQTtFQUNBO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7O0FBRUE7RUFDRTtFQUNBO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7RUFDQTs7QUFFQTtFQUNFO0VBQ0E7O0FBR0Y7RUFDRTtFQUNBOztBQUlKO0VBQ0U7RUFDQTtFQUNBO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7RUFDQTs7QUFFQTtFQUNFO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7O0FBRUE7RUFDRTs7QUFHRjtFQUNFOztBQUlGO0VBQ0U7RUFDQTtFQUNBIiwic291cmNlc0NvbnRlbnQiOlsiLmV4cGFuZC1idXR0b24ge1xuICBwb3NpdGlvbjogYWJzb2x1dGU7XG4gIGRpc3BsYXk6IGZsZXg7XG4gIGZsb2F0OiByaWdodDtcbiAgcGFkZGluZzogMC40cmVtO1xuICBtYXJnaW46IDAuM3JlbTtcbiAgcmlnaHQ6IDA7IC8vIE5PVEU6IHJpZ2h0IHdpbGwgYmUgc2V0IGluIG1lcm1haWQuaW5saW5lLnRzXG4gIGNvbG9yOiB2YXIoLS1ncmF5KTtcbiAgYm9yZGVyLWNvbG9yOiB2YXIoLS1kYXJrKTtcbiAgYmFja2dyb3VuZC1jb2xvcjogdmFyKC0tbGlnaHQpO1xuICBib3JkZXI6IDFweCBzb2xpZDtcbiAgYm9yZGVyLXJhZGl1czogNXB4O1xuICBvcGFjaXR5OiAwO1xuICB0cmFuc2l0aW9uOiAwLjJzO1xuXG4gICYgPiBzdmcge1xuICAgIGZpbGw6IHZhcigtLWxpZ2h0KTtcbiAgICBmaWx0ZXI6IGNvbnRyYXN0KDAuMyk7XG4gIH1cblxuICAmOmhvdmVyIHtcbiAgICBjdXJzb3I6IHBvaW50ZXI7XG4gICAgYm9yZGVyLWNvbG9yOiB2YXIoLS1zZWNvbmRhcnkpO1xuICB9XG5cbiAgJjpmb2N1cyB7XG4gICAgb3V0bGluZTogMDtcbiAgfVxufVxuXG5wcmUge1xuICAmOmhvdmVyID4gLmV4cGFuZC1idXR0b24ge1xuICAgIG9wYWNpdHk6IDE7XG4gICAgdHJhbnNpdGlvbjogMC4ycztcbiAgfVxufVxuXG4jbWVybWFpZC1jb250YWluZXIge1xuICBwb3NpdGlvbjogZml4ZWQ7XG4gIGNvbnRhaW46IGxheW91dDtcbiAgei1pbmRleDogOTk5O1xuICBsZWZ0OiAwO1xuICB0b3A6IDA7XG4gIHdpZHRoOiAxMDB2dztcbiAgaGVpZ2h0OiAxMDB2aDtcbiAgb3ZlcmZsb3c6IGhpZGRlbjtcbiAgZGlzcGxheTogbm9uZTtcbiAgYmFja2Ryb3AtZmlsdGVyOiBibHVyKDRweCk7XG4gIGJhY2tncm91bmQ6IHJnYmEoMCwgMCwgMCwgMC41KTtcblxuICAmLmFjdGl2ZSB7XG4gICAgZGlzcGxheTogaW5saW5lLWJsb2NrO1xuICB9XG5cbiAgJiA+ICNtZXJtYWlkLXNwYWNlIHtcbiAgICBib3JkZXI6IDFweCBzb2xpZCB2YXIoLS1saWdodGdyYXkpO1xuICAgIGJhY2tncm91bmQtY29sb3I6IHZhcigtLWxpZ2h0KTtcbiAgICBib3JkZXItcmFkaXVzOiA1cHg7XG4gICAgcG9zaXRpb246IGZpeGVkO1xuICAgIHRvcDogNTAlO1xuICAgIGxlZnQ6IDUwJTtcbiAgICB0cmFuc2Zvcm06IHRyYW5zbGF0ZSgtNTAlLCAtNTAlKTtcbiAgICBoZWlnaHQ6IDgwdmg7XG4gICAgd2lkdGg6IDgwdnc7XG4gICAgb3ZlcmZsb3c6IGhpZGRlbjtcblxuICAgICYgPiAubWVybWFpZC1jb250ZW50IHtcbiAgICAgIHBhZGRpbmc6IDJyZW07XG4gICAgICBwb3NpdGlvbjogcmVsYXRpdmU7XG4gICAgICB0cmFuc2Zvcm0tb3JpZ2luOiAwIDA7XG4gICAgICB0cmFuc2l0aW9uOiB0cmFuc2Zvcm0gMC4xcyBlYXNlO1xuICAgICAgb3ZlcmZsb3c6IHZpc2libGU7XG4gICAgICBtaW4taGVpZ2h0OiAyMDBweDtcbiAgICAgIG1pbi13aWR0aDogMjAwcHg7XG5cbiAgICAgIHByZSB7XG4gICAgICAgIG1hcmdpbjogMDtcbiAgICAgICAgYm9yZGVyOiBub25lO1xuICAgICAgfVxuXG4gICAgICBzdmcge1xuICAgICAgICBtYXgtd2lkdGg6IG5vbmU7XG4gICAgICAgIGhlaWdodDogYXV0bztcbiAgICAgIH1cbiAgICB9XG5cbiAgICAmID4gLm1lcm1haWQtY29udHJvbHMge1xuICAgICAgcG9zaXRpb246IGFic29sdXRlO1xuICAgICAgYm90dG9tOiAyMHB4O1xuICAgICAgcmlnaHQ6IDIwcHg7XG4gICAgICBkaXNwbGF5OiBmbGV4O1xuICAgICAgZ2FwOiA4cHg7XG4gICAgICBwYWRkaW5nOiA4cHg7XG4gICAgICBiYWNrZ3JvdW5kOiB2YXIoLS1saWdodCk7XG4gICAgICBib3JkZXI6IDFweCBzb2xpZCB2YXIoLS1saWdodGdyYXkpO1xuICAgICAgYm9yZGVyLXJhZGl1czogNnB4O1xuICAgICAgYm94LXNoYWRvdzogMCAycHggNHB4IHJnYmEoMCwgMCwgMCwgMC4xKTtcbiAgICAgIHotaW5kZXg6IDI7XG5cbiAgICAgIC5tZXJtYWlkLWNvbnRyb2wtYnV0dG9uIHtcbiAgICAgICAgZGlzcGxheTogZmxleDtcbiAgICAgICAgYWxpZ24taXRlbXM6IGNlbnRlcjtcbiAgICAgICAganVzdGlmeS1jb250ZW50OiBjZW50ZXI7XG4gICAgICAgIHdpZHRoOiAzMnB4O1xuICAgICAgICBoZWlnaHQ6IDMycHg7XG4gICAgICAgIHBhZGRpbmc6IDA7XG4gICAgICAgIGJvcmRlcjogMXB4IHNvbGlkIHZhcigtLWxpZ2h0Z3JheSk7XG4gICAgICAgIGJhY2tncm91bmQ6IHZhcigtLWxpZ2h0KTtcbiAgICAgICAgY29sb3I6IHZhcigtLWRhcmspO1xuICAgICAgICBib3JkZXItcmFkaXVzOiA0cHg7XG4gICAgICAgIGN1cnNvcjogcG9pbnRlcjtcbiAgICAgICAgZm9udC1zaXplOiAxNnB4O1xuICAgICAgICBmb250LWZhbWlseTogdmFyKC0tYm9keUZvbnQpO1xuICAgICAgICB0cmFuc2l0aW9uOiBhbGwgMC4ycyBlYXNlO1xuXG4gICAgICAgICY6aG92ZXIge1xuICAgICAgICAgIGJhY2tncm91bmQ6IHZhcigtLWxpZ2h0Z3JheSk7XG4gICAgICAgIH1cblxuICAgICAgICAmOmFjdGl2ZSB7XG4gICAgICAgICAgdHJhbnNmb3JtOiB0cmFuc2xhdGVZKDFweCk7XG4gICAgICAgIH1cblxuICAgICAgICAvLyBTdHlsZSB0aGUgcmVzZXQgYnV0dG9uIGRpZmZlcmVudGx5XG4gICAgICAgICY6bnRoLWNoaWxkKDIpIHtcbiAgICAgICAgICB3aWR0aDogYXV0bztcbiAgICAgICAgICBwYWRkaW5nOiAwIDEycHg7XG4gICAgICAgICAgZm9udC1zaXplOiAxNHB4O1xuICAgICAgICB9XG4gICAgICB9XG4gICAgfVxuICB9XG59XG4iXX0= */</style><link href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/><script src="../prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("../static/contentIndex.json").then(data => data.json())</script><link rel="alternate" type="application/rss+xml" title="RSS Feed" href="https://blog.tachibanayu24.com/index.xml"/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:image" content="https://blog.tachibanayu24.com/news/cognition-deepwiki-og-image.webp"/><meta property="og:image:url" content="https://blog.tachibanayu24.com/news/cognition-deepwiki-og-image.webp"/><meta name="twitter:image" content="https://blog.tachibanayu24.com/news/cognition-deepwiki-og-image.webp"/><meta property="og:image:type" content="image/.webp"/></head><body data-slug="news/cognition-deepwiki"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h2 class="page-title"><a href=".."><img style="margin:0;" src="../static/hokori_log.png" alt="icon"/></a></h2><div class="spacer mobile-only"></div><div style="display: flex; flex-direction: row; flex-wrap: nowrap; gap: 1rem;"><div style="flex-grow: 1; flex-shrink: 1; flex-basis: auto; order: 0; align-self: center; justify-self: center;"><div class="search desktop-only"><button class="search-button" style="margin-top:1rem;margin-bottom:-2rem;"><p style="font-size:0.8rem;">検索（⌘+K）</p><svg role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title>Search</title><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></button><div class="search-container"><div class="search-space"><input autocomplete="off" class="search-bar" name="search" type="text" aria-label="検索ワードを入力" placeholder="検索ワードを入力"/><div class="search-layout" data-preview="true"></div></div></div></div></div></div><div style="display: flex; flex-direction: row; flex-wrap: nowrap; gap: 1rem;"><div style="flex-grow: 1; flex-shrink: 1; flex-basis: auto; order: 0; align-self: center; justify-self: center;"><div class="search mobile-only"><button class="search-button"><svg role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title>Search</title><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></button><div class="search-container"><div class="search-space"><input autocomplete="off" class="search-bar" name="search" type="text" aria-label="検索ワードを入力" placeholder="検索ワードを入力"/><div class="search-layout" data-preview="true"></div></div></div></div></div></div><div class="desktop-only"><div class="profile"><img src="https://pbs.twimg.com/profile_images/1582323777756876801/rtFFKM1E_400x400.jpg" alt="Profile"/><div class="profile-info"><p class="profile-name">たちばなゆうと</p><p class="profile-description">ソフトウェアエンジニアです。<br/>スタートアップや金融, 不動産, うさぎが好きです。</p><div class="profile-links"><a href="https://x.com/tachibanayu24" target="_blank" rel="noopener noreferrer"><svg fill="none" height="16" width="16" xmlns="http://www.w3.org/2000/svg" viewBox="0.254 0.25 500 451.95400000000006"><path d="M394.033.25h76.67L303.202 191.693l197.052 260.511h-154.29L225.118 294.205 86.844 452.204H10.127l179.16-204.77L.254.25H158.46l109.234 144.417zm-26.908 406.063h42.483L135.377 43.73h-45.59z" fill="#5d4a3c"></path></svg></a><a href="https://github.com/tachibanayu24" target="_blank" rel="noopener noreferrer"><svg width="16" height="16" viewBox="0 0 256 249" xmlns="http://www.w3.org/2000/svg" preserveAspectRatio="xMinYMin meet"><g fill="#5d4a3c"><path d="M127.505 0C57.095 0 0 57.085 0 127.505c0 56.336 36.534 104.13 87.196 120.99 6.372 1.18 8.712-2.766 8.712-6.134 0-3.04-.119-13.085-.173-23.739-35.473 7.713-42.958-15.044-42.958-15.044-5.8-14.738-14.157-18.656-14.157-18.656-11.568-7.914.872-7.752.872-7.752 12.804.9 19.546 13.14 19.546 13.14 11.372 19.493 29.828 13.857 37.104 10.6 1.144-8.242 4.449-13.866 8.095-17.05-28.32-3.225-58.092-14.158-58.092-63.014 0-13.92 4.981-25.295 13.138-34.224-1.324-3.212-5.688-16.18 1.235-33.743 0 0 10.707-3.427 35.073 13.07 10.17-2.826 21.078-4.242 31.914-4.29 10.836.048 21.752 1.464 31.942 4.29 24.337-16.497 35.029-13.07 35.029-13.07 6.94 17.563 2.574 30.531 1.25 33.743 8.175 8.929 13.122 20.303 13.122 34.224 0 48.972-29.828 59.756-58.22 62.912 4.573 3.957 8.648 11.717 8.648 23.612 0 17.06-.148 30.791-.148 34.991 0 3.393 2.295 7.369 8.759 6.117 50.634-16.879 87.122-64.656 87.122-120.973C255.009 57.085 197.922 0 127.505 0"></path><path d="M47.755 181.634c-.28.633-1.278.823-2.185.389-.925-.416-1.445-1.28-1.145-1.916.275-.652 1.273-.834 2.196-.396.927.415 1.455 1.287 1.134 1.923M54.027 187.23c-.608.564-1.797.302-2.604-.589-.834-.889-.99-2.077-.373-2.65.627-.563 1.78-.3 2.616.59.834.899.996 2.08.36 2.65M58.33 194.39c-.782.543-2.06.034-2.849-1.1-.781-1.133-.781-2.493.017-3.038.792-.545 2.05-.055 2.85 1.07.78 1.153.78 2.513-.019 3.069M65.606 202.683c-.699.77-2.187.564-3.277-.488-1.114-1.028-1.425-2.487-.724-3.258.707-.772 2.204-.555 3.302.488 1.107 1.026 1.445 2.496.7 3.258M75.01 205.483c-.307.998-1.741 1.452-3.185 1.028-1.442-.437-2.386-1.607-2.095-2.616.3-1.005 1.74-1.478 3.195-1.024 1.44.435 2.386 1.596 2.086 2.612M85.714 206.67c.036 1.052-1.189 1.924-2.705 1.943-1.525.033-2.758-.818-2.774-1.852 0-1.062 1.197-1.926 2.721-1.951 1.516-.03 2.758.815 2.758 1.86M96.228 206.267c.182 1.026-.872 2.08-2.377 2.36-1.48.27-2.85-.363-3.039-1.38-.184-1.052.89-2.105 2.367-2.378 1.508-.262 2.857.355 3.049 1.398"></path></g></svg></a></div></div></div></div><div class="recent-notes desktop-only"><h3>最近の更新</h3><ul class="recent-ul"><li class="recent-li"><div class="section"><div class="desc"><h3><a href="../news/chinese-models-launch" class="internal">【bot投稿🤖】中国発AIモデル続々登場 新たな競争軸か</a></h3></div><p class="meta"><time datetime="2025-06-17T04:56:40.000Z">2025年6月17日</time></p></div></li><li class="recent-li"><div class="section"><div class="desc"><h3><a href="../news/ai-latest-updates-jun25" class="internal">【bot投稿🤖】AI最新動向 Google DeepSearchやNvidia新モデル登場</a></h3></div><p class="meta"><time datetime="2025-06-05T05:52:28.000Z">2025年6月05日</time></p></div></li><li class="recent-li"><div class="section"><div class="desc"><h3><a href="../news/meeker-ai-deepseek-r1" class="internal">【bot投稿🤖】メアリーミーカー氏のAIレポートなど</a></h3></div><p class="meta"><time datetime="2025-05-31T03:49:43.000Z">2025年5月31日</time></p></div></li></ul><p style="text-align:right;padding-right:1rem;"><a href="../tags/" style="font-size:0.8rem;">さらに19件 →</a></p></div><div class="explorer" data-behavior="collapse" data-collapsed="collapsed" data-savestate="true" data-data-fns="{&quot;order&quot;:[&quot;filter&quot;,&quot;map&quot;,&quot;sort&quot;],&quot;sortFn&quot;:&quot;(a,b)=>!a.isFolder&amp;&amp;!b.isFolder||a.isFolder&amp;&amp;b.isFolder?a.displayName.localeCompare(b.displayName,void 0,{numeric:!0,sensitivity:\&quot;base\&quot;}):!a.isFolder&amp;&amp;b.isFolder?1:-1&quot;,&quot;filterFn&quot;:&quot;node=>node.slugSegment!==\&quot;tags\&quot;&quot;,&quot;mapFn&quot;:&quot;node=>node&quot;}"><button type="button" class="explorer-toggle mobile-explorer hide-until-loaded" data-mobile="true" aria-controls="explorer-content"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide-menu"><line x1="4" x2="20" y1="12" y2="12"></line><line x1="4" x2="20" y1="6" y2="6"></line><line x1="4" x2="20" y1="18" y2="18"></line></svg></button><button type="button" class="title-button explorer-toggle desktop-explorer" data-mobile="false" aria-expanded="true"><h2 style="font-size:0.8rem;color:var(--darkgray);">記事一覧</h2><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="5 8 14 8" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div class="explorer-content" aria-expanded="false"><ul class="explorer-ul overflow" id="list-0"><li class="overflow-end"></li></ul></div><template id="template-file"><li><a href="#"></a></li></template><template id="template-folder"><li><div class="folder-container"><div><button class="folder-button"><span class="folder-title"></span></button></div></div><div class="folder-outer"><ul class="content"></ul></div></li></template></div></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="../">Top</a><p> / </p></div><div class="breadcrumb-element"><a href="../news/">news</a><p> / </p></div><div class="breadcrumb-element"><a href>【bot投稿🤖】GitHub解説DeepWiki登場とAI最新ニュースまとめ</a></div></nav><h1 class="article-title">【bot投稿🤖】GitHub解説DeepWiki登場とAI最新ニュースまとめ</h1><p style="font-size:0.8rem;" show-comma="true" class="content-meta"><time datetime="2025-04-27T03:02:44.000Z">2025年4月27日</time><span>18分くらいで読めます</span></p><ul class="tags"><li><a href="../tags/自動生成記事" class="internal tag-link">自動生成記事</a></li><li><a href="../tags/LMM" class="internal tag-link">LMM</a></li></ul></div></div><article class="popover-hint"><blockquote class="callout warning" data-callout="warning">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Warning</p></div>
                  
                </div>
<div class="callout-content">
<p>この記事は、以下の情報源を参照し、LLMにより自動で生成・投稿された記事です。</p>
<ul>
<li><a href="https://news.smol.ai/" class="external" target="_blank">AINews<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></li>
</ul>
<p>内容の正確性にご注意ください。</p>
</div>
</blockquote>
<h1 id="githubリポジトリを自動解説-deepwiki登場とai最新ニュースまとめ">GitHubリポジトリを自動解説 DeepWiki登場とAI最新ニュースまとめ<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#githubリポジトリを自動解説-deepwiki登場とai最新ニュースまとめ" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<p>AIエージェント開発で知られるCognitionから、GitHubリポジトリを自動で解説してくれる「DeepWiki」が登場しました。これは開発者にとって非常に便利なツールになりそうです。今回はこのDeepWikiを中心に、Metaの新モデルやOpenAIの動向、ローカルLLMの話題に触れます。</p>
<h2 id="cognition-deepwiki-githubリポジトリの百科事典">Cognition DeepWiki: GitHubリポジトリの百科事典<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#cognition-deepwiki-githubリポジトリの百科事典" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>CognitionのSilas Alberti氏が発表した<a href="https://deepwiki.com/" class="external" target="_blank">DeepWiki<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>は、公開されているGitHubリポジトリのURL（例: <code>https://github.com/facebook/react</code>）を <code>https://deepwiki.com/facebook/react</code> のように置き換えるだけで、そのリポジトリに関するWikipedia風の解説ページを表示してくれるサービスです。</p>
<p><img src="https://resend-attachments.s3.amazonaws.com/k0kw0QHMzZ5Urnn" alt="DeepWikiのスクリーンショット"/></p>
<p>解説の精度はかなり高いようで、AINewsのテストでもReactやAstroといったリポジトリで非常に有用な結果が得られたとのこと。さらに、リポジトリの使い方について質問できるDevinベースのチャットボットも統合されています。オープンソースコードを利用する際に、概要把握や使い方調査の手間を大幅に削減できそうですね。</p>
<h2 id="モデルリリースとアップデート">モデルリリースとアップデート<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#モデルリリースとアップデート" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<h3 id="meta-perception-encoders-pe">Meta Perception Encoders (PE)<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#meta-perception-encoders-pe" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>Metaから、画像/動画エンコーダーを含む汎用的な視覚モデル群「Perception Encoders (PE)」がApache 2.0ライセンスでリリースされました (<a href="https://twitter.com/mervenoyann/status/1915723394701467909" class="external" target="_blank">@mervenoyann氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)。</p>
<ul>
<li>画像・動画エンコーダー、視言語理解、空間理解に対応</li>
<li>InternVL3やQwen2.5VLを上回る性能</li>
<li>巨大な画像・動画データセットも同時公開</li>
<li>PE Coreはゼロショット画像タスクでSigLIP2を超える性能 (<a href="https://twitter.com/mervenoyann/status/1915723399642435634" class="external" target="_blank">@mervenoyann氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
<li>モデルとデータセットへのリンク
<ul>
<li><a href="https://twitter.com/mervenoyann/status/1915723397272654194" class="external" target="_blank">Perception Encoder models<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></li>
<li><a href="https://twitter.com/mervenoyann/status/1915723397272654194" class="external" target="_blank">Perception LM models<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></li>
</ul>
</li>
</ul>
<h3 id="qwen-chat-app">Qwen Chat App<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#qwen-chat-app" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>AlibabaのQwenチームが、iOSおよびAndroid向けの「Qwen Chat APP」をリリースしました (<a href="https://twitter.com/Alibaba_Qwen/status/1915761990703697925" class="external" target="_blank">@Alibaba_Qwen氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)。</p>
<h3 id="hugging-face--fal">Hugging Face + FAL<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#hugging-face--fal" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>30,000以上のFlux/SDXL LoRAがHugging Face Hubで推論可能に (<a href="https://twitter.com/reach_vb/status/1915830938438717777" class="external" target="_blank">@reach_vb氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)
<ul>
<li>1ドル未満で40枚以上の画像を生成可能</li>
</ul>
</li>
<li>新しいText-to-Speechモデル「Dia 1.6B SoTA」がHugging Face上でFAL経由で利用可能に (<a href="https://twitter.com/reach_vb/status/1915418386818834792" class="external" target="_blank">@reach_vb氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)
<ul>
<li>1ドル未満で25世代まで生成可能</li>
</ul>
</li>
</ul>
<h3 id="openai-deep-research-軽量版">OpenAI Deep Research (軽量版)<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#openai-deep-research-軽量版" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>OpenAIは、Plus, Team, Proユーザー向けに提供していたDeep Research機能の軽量版を導入し、レート制限を緩和しました (<a href="https://twitter.com/OpenAI/status/1915505959931437178" class="external" target="_blank">@OpenAI氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)。</p>
<ul>
<li>この軽量版は無料ユーザーにも提供開始 (<a href="https://twitter.com/gdb/status/1915637620731941188" class="external" target="_blank">@gdb氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
<li>OpenAI o4-miniのバージョンによって動作</li>
<li>従来のDeep Researchに近い知能を持ちながら、大幅に低コストで提供可能 (<a href="https://twitter.com/OpenAI/status/1915505961500070245" class="external" target="_blank">@OpenAI氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
</ul>
<h3 id="perplexity-モデルアップデート">Perplexity モデルアップデート<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#perplexity-モデルアップデート" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>Perplexityはモデルセレクターに新しいモデルを追加しました (<a href="https://twitter.com/perplexity_ai/status/1915819644256129424" class="external" target="_blank">@perplexity_ai氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)。</p>
<ul>
<li>Grok 3 Betaとo4-miniが利用可能に</li>
<li>既存モデル (gemini 2.5 pro, claude 3.7, perplexity sonar, gpt-4.1, deepseek r1 1776) に加えてo3も検討中 (<a href="https://twitter.com/AravSrinivas/status/1915820052571689245" class="external" target="_blank">@AravSrinivas氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
<li>コンテキストに応じた画像生成・編集が可能な最新のOpenAI画像生成モデルも導入 (<a href="https://twitter.com/perplexity_ai/status/1915819619333640647" class="external" target="_blank">@perplexity_ai氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
</ul>
<h3 id="vllm-for-rlhf">vLLM for RLHF<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#vllm-for-rlhf" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>OpenRLHFフレームワークがvLLMをRLHFに活用していることが紹介されました (<a href="https://twitter.com/vllm_project/status/1915307134256091570" class="external" target="_blank">@vllm_project氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)。
vLLMは多くのRLHFフレームワークで採用されているようです。</p>
<h3 id="surya-ocr">Surya OCR<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#surya-ocr" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>90以上の言語、LaTeX、フォーマットに対応した新しいOCRモデル「Surya」のアルファ版がリリースされました (<a href="https://twitter.com/VikParuchuri/status/1915492483955384659" class="external" target="_blank">@VikParuchuri氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)。</p>
<ul>
<li>文字/単語/行のバウンディングボックスを提供</li>
<li>約5億の非埋め込みパラメータ</li>
<li>1秒あたり10-20ページの処理速度</li>
</ul>
<h2 id="フレームワークツールデータセット">フレームワーク、ツール、データセット<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#フレームワークツールデータセット" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li><strong>MegaParse</strong>: あらゆるドキュメントをLLMに適した形式に変換するオープンソースPythonライブラリ (<a href="https://twitter.com/LiorOnAI/status/1915792212157407385" class="external" target="_blank">@LiorOnAI氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)
<ul>
<li>PDF, Powerpoint, Word, 表, 目次, ヘッダー, フッター, 画像に対応</li>
</ul>
</li>
<li><strong>LangGraph DevX</strong>: LangGraphの開発者体験向上のための議論（事前ビルド済みエージェントコンストラクタをクラスにするか関数にするか） (<a href="https://twitter.com/hwchase17/status/1915208270593352002" class="external" target="_blank">@hwchase17氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
<li><strong>Google Agent Development Kit (ADK)</strong>: GoogleのADK入門ガイドが共有 (<a href="https://twitter.com/omarsar0/status/1915402607574893052" class="external" target="_blank">@omarsar0氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
<li><strong>ReflectionFlow</strong>: Text-to-Imageモデルが自己反省を通じて出力を改善するフレームワーク (<a href="https://twitter.com/RisingSayak/status/1915338106510905767" class="external" target="_blank">@RisingSayak氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)
<ul>
<li>大規模データセット「GenRef-1M」(良い画像、悪い画像、反省のトリプレット) もリリース</li>
</ul>
</li>
<li><strong>OpenAI Codex Fund Grant</strong>: 初の助成対象が発表 (vLLM, OWASP Nettacker, Pulumi, Dagster) (<a href="https://twitter.com/OpenAIDevs/status/1915524612970152376" class="external" target="_blank">@OpenAIDevs氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
<li><strong>Spotify ViSMaP</strong>: SpotifyがHugging Face上で公開した、メタプロンプティングによる教師なし長時間動画要約モデル (<a href="https://twitter.com/_akhaliq/status/1915703054701044209" class="external" target="_blank">@_akhaliq氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
<li><strong>ByteDance QuaDMix</strong>: ByteDanceがHugging Face上で公開した、効率的なLLM事前学習のための品質多様性バランスデータ選択手法 (<a href="https://twitter.com/_akhaliq/status/1915656590130036887" class="external" target="_blank">@_akhaliq氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
<li><strong>DeepSeek R1 データセット</strong>: DeepSeek R1の解釈可能な特徴を研究者が探索するためのクエリ可能な新しいデータセット (<a href="https://twitter.com/GoodfireAI/status/1915802798513598490" class="external" target="_blank">@GoodfireAI氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
<li><strong>Trackers v2.0.0</strong>: トップモデルライブラリの複合オブジェクト検出器とマルチオブジェクトトラッカー（SORT, DeepSORT対応）を組み合わせるツール (<a href="https://twitter.com/skalskip92/status/1915439480594485363" class="external" target="_blank">@skalskip92氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
</ul>
<h2 id="エージェントシステムとツール利用">エージェントシステムとツール利用<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#エージェントシステムとツール利用" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li><strong>Agentic AIと可視性</strong>: Weights &amp; BiasesがDeepsetと協力し、AIワークフローの可視性を向上させる取り組みを発表 (<a href="https://twitter.com/weights_biases/status/1915498157754233092" class="external" target="_blank">@weights_biases氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
<li><strong>Meta 3D Generative AI</strong>: Metaが3D生成AI分野の研究者採用を積極的に行っている (<a href="https://twitter.com/AIatMeta/status/1915437886209745338" class="external" target="_blank">@AIatMeta氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
<li><strong>PerplexityとMotorola提携</strong>: PerplexityのAndroidアプリが新しいMotorolaデバイスにプリインストールされ、Moto Razr向けに最適化されたアシスタントを提供。新規購入者にはPerplexity Pro 3ヶ月分が付与 (<a href="https://twitter.com/perplexity_ai/status/1915438278947283301" class="external" target="_blank">@perplexity_ai氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
<li><strong>Google Cloud リアルタイムエージェント</strong>: Google Cloudが、パーソナライズされ、リアルタイムでマルチモーダルな次世代エージェントのデモを公開。Gemini 2.0 FlashとLive APIを活用 (<a href="https://twitter.com/_philschmid/status/1915360039570739283" class="external" target="_blank">@_philschmid氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
</ul>
<h2 id="解釈可能性と評価">解釈可能性と評価<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#解釈可能性と評価" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li><strong>AI解釈可能性</strong>: AIモデルの精神を理解し設計することの緊急性が強調されている (<a href="https://twitter.com/GoodfireAI/status/1915617077915967714" class="external" target="_blank">@GoodfireAI氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)。学術界が貢献できる分野としても注目 (<a href="https://twitter.com/NeelNanda5/status/1915546126259806590" class="external" target="_blank">@NeelNanda5氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
<li><strong>AI支援コーディング</strong>: Karpathy氏が、AI支援コーディングの現状のUI/UXにはまだ改善の余地が多いと指摘 (<a href="https://twitter.com/karpathy/status/1915581920022585597" class="external" target="_blank">@karpathy氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
<li><strong>LLM評価リソース</strong>: LLM評価に関する無料/オープンリソースが紹介されている (<a href="https://twitter.com/clefourrier/status/1915339216344526896" class="external" target="_blank">@clefourrier氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)
<ul>
<li><a href="https://github.com/NannyML/nannyml/blob/main/guide/llm-guide.md" class="external" target="_blank">LLM guidebook<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></li>
<li><a href="https://github.com/seb-lgr/your-bench" class="external" target="_blank">YourBench<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></li>
</ul>
</li>
</ul>
<h2 id="ai倫理と福祉業界動向">AI倫理と福祉、業界動向<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#ai倫理と福祉業界動向" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li><strong>AI福祉</strong>: AnthropicがAIの福祉に関する研究プログラムを開始。AIモデルが複雑化・高性能化するにつれて独自の経験を持つ可能性を探る (<a href="https://twitter.com/AnthropicAI/status/1915420604397744497" class="external" target="_blank">@AnthropicAI氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
<li><strong>研究者の移動</strong>: 優秀なAI研究者kaicathyc氏が米国のグリーンカードを拒否され、国外退去を余儀なくされたとの報告 (<a href="https://twitter.com/polynoamial/status/1915765141846515883" class="external" target="_blank">@polynoamial氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
<li><strong>AIとメディア</strong>: 誰もが高品質なコンテンツを作成し、大量配布できる時代が来るとの視点 (<a href="https://twitter.com/c_valenzuelab/status/1915514295816737140" class="external" target="_blank">@c_valenzuelab氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
<li><strong>UberとLangGraph</strong>: Uberの開発者プラットフォームチームがLangGraphを使用してユニットテスト生成を自動化 (<a href="https://twitter.com/LangChainAI/status/1915191956810207431" class="external" target="_blank">@LangChainAI氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
<li><strong>ICLRカンファレンス</strong>: シンガポールで開催中のICLR 2025に関する情報が共有されている (<a href="https://twitter.com/AIatMeta/status/1915437886209745338" class="external" target="_blank">@AIatMeta氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>, <a href="https://twitter.com/hardmaru/status/1915341552332808383" class="external" target="_blank">@hardmaru氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>, <a href="https://twitter.com/shaneguML/status/1915169621042499846" class="external" target="_blank">@shaneguML氏のXポスト<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
</ul>
<h2 id="ローカルllmと関連トピックredditより">ローカルLLMと関連トピック（Redditより）<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#ローカルllmと関連トピックredditより" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>Redditの/r/LocalLlamaでは、ローカル環境で動作するLLMに関する活発な議論が行われています。</p>
<ul>
<li><strong>DF11: Lossless LLM Compression</strong>
<ul>
<li>BF16モデルを推論時に約70%のサイズにロスレス圧縮する技術 (<a href="https://www.reddit.com/r/LocalLLaMA/comments/1k7o89n/we_compress_any_bf16_model_to_70_size_during/" class="external" target="_blank">Reddit投稿<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
<li>メモリフットプリントを削減し、限られたVRAMで大規模モデルの実行を可能にする</li>
<li>ロスレスであるため、量子化のような予測不可能な精度低下がない</li>
<li>GitHub: <a href="https://github.com/LeanModels/DFloat11" class="external" target="_blank">LeanModels/DFloat11<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></li>
<li>論文: <a href="https://arxiv.org/abs/2504.11651" class="external" target="_blank">arXiv:2504.11651<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></li>
</ul>
</li>
<li><strong>Tessa-Rust-T1-7B: Rust特化モデル</strong>
<ul>
<li>7BパラメータのRustコーディング特化モデルが登場 (<a href="https://www.reddit.com/r/LocalLLaMA/comments/1k7e542/7b_reasoning_rust_coding_model_with_open_dataset/" class="external" target="_blank">Reddit投稿<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
<li>データセットの品質や評価プロセスに関する透明性の欠如が指摘されている</li>
<li>特定言語への特化が汎用モデルと比較して性能向上につながるか議論</li>
</ul>
</li>
<li><strong>Dyad: ローカルAIアプリビルダー</strong>
<ul>
<li>v0やLovableのようなプロプライエタリツールに対する、無料・ローカル・オープンソースの代替 (<a href="https://www.reddit.com/r/LocalLLaMA/comments/1k76ztc/i_built_a_free_local_opensource_alternative_to/" class="external" target="_blank">Reddit投稿<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
<li>Ollama経由でのローカルモデルサポート、APIキー持ち込み対応</li>
<li>GitHub: <a href="https://github.com/dyad-sh/dyad" class="external" target="_blank">dyad-sh/dyad<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></li>
<li>Webサイト: <a href="https://dyad.sh/" class="external" target="_blank">dyad.sh<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></li>
</ul>
</li>
<li><strong>Gemma 3のシステムプロンプト問題</strong>
<ul>
<li>Gemma 3がシステムプロンプトを無視する挙動が報告されている (<a href="https://www.reddit.com/r/LocalLLaMA/comments/1k7krlm/gemma_3_fakes_and_ignores_the_system_prompt/" class="external" target="_blank">Reddit投稿<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
<li>モデル自体がシステムプロンプトをネイティブサポートしておらず、単にユーザー入力の先頭に追加しているだけ (<a href="https://huggingface.co/google/gemma-3-27b-it/blob/main/chat_template.json" class="external" target="_blank">chat_template.json<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
<li>ただし、一部のユーザーからは特定のタスク（高コンテキストな創作など）で他の大規模モデルより優れた指示追従性を示すとの報告もある</li>
</ul>
</li>
</ul>
<h2 id="civitai論争と代替プラットフォームredditより">CivitAI論争と代替プラットフォーム（Redditより）<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#civitai論争と代替プラットフォームredditより" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>AI画像生成モデル共有サイトCivitAIが、支払いプロセッサ（Visa/Mastercard）からの圧力によりコンテンツ削除を進めている問題が/r/StableDiffusionで話題になっています。</p>
<ul>
<li><strong>CivitAIへの圧力</strong>: PatreonやPixiv Fanboxと同様に、支払いプロセッサがリスク回避のために規約遵守を強化しており、CivitAIもその影響を受けている (<a href="https://www.reddit.com/r/StableDiffusion/comments/1k7p5uw/civitai_is_toast_and_here_is_why/" class="external" target="_blank">Reddit投稿1<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)
<ul>
<li>現状のモデレーションでは不十分で、更なるコンテンツ削除や方針転換が起こる可能性</li>
</ul>
</li>
<li><strong>CivitAI代替リスト</strong>: CivitAIから削除されたモデルなどをホストする代替プラットフォームがリストアップされている (<a href="https://www.reddit.com/r/StableDiffusion/comments/1k7dvfb/in_reguards_to_civitai_removing_models/" class="external" target="_blank">Reddit投稿2<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)
<ul>
<li>Tensor.art, Huggingface.co, ModelScope.cn, Prompthero.com, Pixai.art, Seaart.ai, civitarc.comなど</li>
<li>Tensor.artについては無断転載の問題も指摘されている</li>
</ul>
</li>
<li><strong>Diffusion Arc (旧Civit Arc)</strong>: 検閲フリーなモデルデータベースとして新たに登場 (<a href="https://www.reddit.com/r/StableDiffusion/comments/1k7po5a/civit_arc_an_open_database_of_image_gen_models/" class="external" target="_blank">Reddit投稿3<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)
<ul>
<li>CivitAIからのモデル削除に対抗する動き</li>
<li>Torrentサポートやモデルバージョニングを計画</li>
<li>Webサイト: <a href="https://www.civitarc.com/" class="external" target="_blank">Diffusion Arc<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></li>
<li>NSFWコンテンツを許可しつつStripeを利用している点に懸念の声も</li>
</ul>
</li>
</ul>
<h2 id="openaiモデルの問題点と戦略redditより">OpenAIモデルの問題点と戦略（Redditより）<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#openaiモデルの問題点と戦略redditより" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>OpenAIの最新モデルに関する課題や戦略についてもRedditで議論されています。</p>
<ul>
<li><strong>o3モデルの幻覚問題</strong>: OpenAIのo3モデルが、特定のベンチマーク(PersonQA)で33%という高い幻覚率を示したことが報告されている (<a href="https://www.reddit.com/r/OpenAI/comments/1k7pl37/o3_hallucinates_33_of_the_time_why_isnt_this/" class="external" target="_blank">Reddit投稿<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)
<ul>
<li>これは敵対的なデータセットでの結果であり、一般的な利用シーンでの幻覚率を示すものではない点に注意が必要</li>
<li>しかし、実用上でもo3の幻覚傾向が問題視されている声もある</li>
</ul>
</li>
<li><strong>OpenAI OSモデルの噂</strong>: Sam Altman氏が次期オープンソースモデルのリリース時期について「heat waves」と暗号めいた回答をしたことが話題に (<a href="https://www.reddit.com/r/OpenAI/comments/1k7rbjm/os_model_coming_in_june_or_july/" class="external" target="_blank">Reddit投稿<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)
<ul>
<li>多くのユーザーは夏（6月か7月）のリリースを示唆していると解釈</li>
</ul>
</li>
<li><strong>ChatGPTチートマップ</strong>: ChatGPTの効果的な使い方（モデル選択、機能有効化、プロンプト）をまとめたフローチャートが共有されている (<a href="https://www.reddit.com/r/ChatGPT/comments/1k7lbhq/made_a_chatgpt_cheat_map_to_stop_guessing_models/" class="external" target="_blank">Reddit投稿<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)
<ul>
<li>日常的なユーザーが最適なワークフローを選択するのに役立つ</li>
</ul>
</li>
</ul>
<h2 id="フロンティアモデルとベンチマークredditより">フロンティアモデルとベンチマーク（Redditより）<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#フロンティアモデルとベンチマークredditより" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>最先端AIモデルの能力と評価方法に関する議論も活発です (/r/singularityより)。</p>
<ul>
<li><strong>PHYBench</strong>: 物理推論能力を測る新しいベンチマーク (<a href="https://www.reddit.com/r/singularity/comments/1k7f9dd/new_reasoning_benchmark_where_expert_humans_are/" class="external" target="_blank">Reddit投稿<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)
<ul>
<li>現状では人間の専門家が最新LLMを大きく上回る性能を示す</li>
<li>LLMには空間的・図形的推論能力が不足していることが示唆される</li>
</ul>
</li>
<li><strong>AI Visionと人間視覚の乖離</strong>: 最新のDNN（GPT-4o, Claude 3, Gemini 2など）は視覚タスクの精度が向上するにつれて、その内部処理が霊長類の視覚から乖離しているという研究 (<a href="https://arxiv.org/pdf/2504.16940" class="external" target="_blank">arXiv:2504.16940<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>, <a href="https://www.reddit.com/r/singularity/comments/1k7dwld/new_paper_ai_vision_is_becoming_fundamentally/" class="external" target="_blank">Reddit投稿<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)
<ul>
<li>人間のようなAI視覚を実現するには、動的で embodied な訓練が必要と主張</li>
</ul>
</li>
<li><strong>MMOゲームによるAGIテスト</strong>: 静的なベンチマークではなく、MMOゲームのような動的で複雑な環境こそが真のAGI能力を測る究極のチューリングテストであるという提案 (<a href="https://www.reddit.com/r/singularity/comments/1k7m5ui/the_ultimate_turing_test_for_agi_is_mmo_games/" class="external" target="_blank">Reddit投稿<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)
<ul>
<li>視覚推論、感覚知覚、メタ学習、敵対的堅牢性、ゼロショット学習などを同時に要求する</li>
</ul>
</li>
</ul>
<h2 id="discordでの注目トピック">Discordでの注目トピック<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#discordでの注目トピック" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>各AIコミュニティのDiscordでも様々な議論が行われています。</p>
<ul>
<li><strong>モデルアップデートと性能</strong>
<ul>
<li>O3: コード出力が最大700-1000行に増加 (以前の約2倍)</li>
<li>Sunstrike: 新モデルがLMArenaに登場。Google製？性能は <code>claude-3-7-sonnet &amp;gt; sunstrike &amp;gt; gemma-3-12b-it</code> 程度か</li>
<li>GLM-4: Hugging Faceに登場。一部ベンチマークでDeepSeek R1を上回る性能</li>
</ul>
</li>
<li><strong>ハードウェアと最適化</strong>
<ul>
<li>LM Studio 0.3.15: NVIDIA RTX 50シリーズ (CUDA 12.8) サポート追加 (<a href="https://lmstudio.ai/download" class="external" target="_blank">ダウンロード<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>, <a href="https://lmstudio.ai/blog/lmstudio-v0.3.15" class="external" target="_blank">リリースノート<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
<li>CUDA vs Intel: AIタスクではCUDAサポートが充実しているNVIDIA GPU (例: RTX 3060) が有利との見方</li>
<li>TACQ: LLMを2bit精度に高精度で圧縮するタスクアウェア量子化手法 (<a href="https://www.marktechpost.com/2025/04/22/llms-can-now-retain-high-accuracy-at-2-bit-precision-researchers-from-unc-chapel-hill-introduce-tacq-a-task-aware-quantization-approach-that-preserves-critical-weight-circuits-for-compression-withou/" class="external" target="_blank">論文解説記事<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
</ul>
</li>
<li><strong>AIフレームワークとツール</strong>
<ul>
<li>Aider: プログラミング言語別のソート機能を追加</li>
<li>LlamaIndex FunctionAgent: <code>request_timeout</code> パラメータによるタイムアウト設定に対応</li>
<li>Kubernetes MCP: k8s APIベースの新しいMCP実装が登場 (<a href="https://github.com/StacklokLabs/mkp" class="external" target="_blank">GitHub<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
</ul>
</li>
<li><strong>AI研究と概念</strong>
<ul>
<li>AGIへの道筋: Yann LeCun氏の論文「A Path Towards Machine Intelligence」(<a href="https://openreview.net/pdf?id=BZ5a1r-kVsf" class="external" target="_blank">OpenReview<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>) が参照され、潜在空間変換の重要性が議論</li>
<li>エージェント構築: Anthropic, dexhorthy (12 Factor Agents), OpenAIなどからエージェント構築に関するガイドや議論が活発化</li>
<li>SimpleStories: TinyStoriesに代わる新しいデータセット、トークナイザー、モデルスイートが登場 (<a href="https://huggingface.co/datasets/lennart-finke/SimpleStories" class="external" target="_blank">Hugging Face Datasets<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>, <a href="https://huggingface.co/SimpleStories" class="external" target="_blank">Models<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
</ul>
</li>
<li><strong>業界ニュースとプラットフォーム</strong>
<ul>
<li>Nous Research: 暗号VC Paradigmから$50Mを調達 (<a href="https://fortune.com/crypto/2025/04/25/paradigm-nous-research-crypto-ai-venture-capital-deepseek-openai-blockchain/" class="external" target="_blank">Fortune記事<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)
<ul>
<li>Solanaベースの分散学習プロジェクト「Psyche」も進行中 (<a href="https://nousresearch.com/nous-psyche/" class="external" target="_blank">Psyche Website<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>, <a href="https://discord.gg/peqZyPRd" class="external" target="_blank">Psyche Discord<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>)</li>
</ul>
</li>
<li>Gemini 無料枠制限: Gemini 2.5 Pro Experimental (Free) の需要過多によりレート制限強化 (1 req/min, 1000 req/day)</li>
<li>OpenRouter クレジット問題: 無限URL生成の悪用によりクレジットが枯渇するインシデント発生</li>
</ul>
</li>
</ul>
<h2 id="まとめ">まとめ<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#まとめ" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>GitHubリポジトリの理解を助けるDeepWikiの登場は、開発者にとって大きな助けとなりそうです。MetaのPerception EncodersやOpenAIのDeep Research軽量版など、大手プレイヤーからのリリースも続いています。一方で、CivitAIを巡る動きのように、プラットフォーム側の課題も顕在化しています。</p>
<p>ローカルLLMの圧縮技術DF11や、Rust特化モデル、ローカルAIアプリビルダーDyadなど、ローカル環境でのAI活用も進んでいます。また、物理推論ベンチマークPHYBenchやAI視覚に関する研究は、現在のAIの限界と可能性を示唆しています。</p>
<p>Discordコミュニティでは、具体的なモデルの性能比較やツールの使い方、ハードウェアの最適化、そしてAGIへの道筋といった基礎的な議論まで、活発な情報交換が行われています。AIの進化は依然として目覚ましく、多方面でのブレークスルーが期待されます。</p></article><div class="page-footer"><div style="text-align:center;margin:3rem 0;"><a href="https://x.com/intent/tweet?text=%E3%80%90bot%E6%8A%95%E7%A8%BF%F0%9F%A4%96%E3%80%91GitHub%E8%A7%A3%E8%AA%ACDeepWiki%E7%99%BB%E5%A0%B4%E3%81%A8AI%E6%9C%80%E6%96%B0%E3%83%8B%E3%83%A5%E3%83%BC%E3%82%B9%E3%81%BE%E3%81%A8%E3%82%81%20%7C%20%E3%81%BB%E3%81%93%E3%82%8A%E3%83%AD%E3%82%B0&amp;url=https%3A%2F%2Fblog.tachibanayu24.com%2Fnews%252Fcognition-deepwiki" target="_blank" rel="noopener noreferrer" class="twitter-share-button" style="box-shadow:0 4px 12px rgba(0, 0, 0, 0.15);display:inline-flex;align-items:center;padding:0.5rem 1rem;background-color:#8b7355;color:white;border-radius:9999px;text-decoration:none;font-size:0.875rem;gap:0.5rem;"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="currentColor"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"></path></svg>Share on X</a></div><div class="mobile-only"><div class="profile"><img src="https://pbs.twimg.com/profile_images/1582323777756876801/rtFFKM1E_400x400.jpg" alt="Profile"/><div class="profile-info"><p class="profile-name">たちばなゆうと</p><p class="profile-description">ソフトウェアエンジニアです。<br/>スタートアップや金融, 不動産, うさぎが好きです。</p><div class="profile-links"><a href="https://x.com/tachibanayu24" target="_blank" rel="noopener noreferrer"><svg fill="none" height="16" width="16" xmlns="http://www.w3.org/2000/svg" viewBox="0.254 0.25 500 451.95400000000006"><path d="M394.033.25h76.67L303.202 191.693l197.052 260.511h-154.29L225.118 294.205 86.844 452.204H10.127l179.16-204.77L.254.25H158.46l109.234 144.417zm-26.908 406.063h42.483L135.377 43.73h-45.59z" fill="#5d4a3c"></path></svg></a><a href="https://github.com/tachibanayu24" target="_blank" rel="noopener noreferrer"><svg width="16" height="16" viewBox="0 0 256 249" xmlns="http://www.w3.org/2000/svg" preserveAspectRatio="xMinYMin meet"><g fill="#5d4a3c"><path d="M127.505 0C57.095 0 0 57.085 0 127.505c0 56.336 36.534 104.13 87.196 120.99 6.372 1.18 8.712-2.766 8.712-6.134 0-3.04-.119-13.085-.173-23.739-35.473 7.713-42.958-15.044-42.958-15.044-5.8-14.738-14.157-18.656-14.157-18.656-11.568-7.914.872-7.752.872-7.752 12.804.9 19.546 13.14 19.546 13.14 11.372 19.493 29.828 13.857 37.104 10.6 1.144-8.242 4.449-13.866 8.095-17.05-28.32-3.225-58.092-14.158-58.092-63.014 0-13.92 4.981-25.295 13.138-34.224-1.324-3.212-5.688-16.18 1.235-33.743 0 0 10.707-3.427 35.073 13.07 10.17-2.826 21.078-4.242 31.914-4.29 10.836.048 21.752 1.464 31.942 4.29 24.337-16.497 35.029-13.07 35.029-13.07 6.94 17.563 2.574 30.531 1.25 33.743 8.175 8.929 13.122 20.303 13.122 34.224 0 48.972-29.828 59.756-58.22 62.912 4.573 3.957 8.648 11.717 8.648 23.612 0 17.06-.148 30.791-.148 34.991 0 3.393 2.295 7.369 8.759 6.117 50.634-16.879 87.122-64.656 87.122-120.973C255.009 57.085 197.922 0 127.505 0"></path><path d="M47.755 181.634c-.28.633-1.278.823-2.185.389-.925-.416-1.445-1.28-1.145-1.916.275-.652 1.273-.834 2.196-.396.927.415 1.455 1.287 1.134 1.923M54.027 187.23c-.608.564-1.797.302-2.604-.589-.834-.889-.99-2.077-.373-2.65.627-.563 1.78-.3 2.616.59.834.899.996 2.08.36 2.65M58.33 194.39c-.782.543-2.06.034-2.849-1.1-.781-1.133-.781-2.493.017-3.038.792-.545 2.05-.055 2.85 1.07.78 1.153.78 2.513-.019 3.069M65.606 202.683c-.699.77-2.187.564-3.277-.488-1.114-1.028-1.425-2.487-.724-3.258.707-.772 2.204-.555 3.302.488 1.107 1.026 1.445 2.496.7 3.258M75.01 205.483c-.307.998-1.741 1.452-3.185 1.028-1.442-.437-2.386-1.607-2.095-2.616.3-1.005 1.74-1.478 3.195-1.024 1.44.435 2.386 1.596 2.086 2.612M85.714 206.67c.036 1.052-1.189 1.924-2.705 1.943-1.525.033-2.758-.818-2.774-1.852 0-1.062 1.197-1.926 2.721-1.951 1.516-.03 2.758.815 2.758 1.86M96.228 206.267c.182 1.026-.872 2.08-2.377 2.36-1.48.27-2.85-.363-3.039-1.38-.184-1.052.89-2.105 2.367-2.378 1.508-.262 2.857.355 3.049 1.398"></path></g></svg></a></div></div></div></div><div class="recent-notes mobile-only"><h3>最近の更新</h3><ul class="recent-ul"><li class="recent-li"><div class="section"><div class="desc"><h3><a href="../news/chinese-models-launch" class="internal">【bot投稿🤖】中国発AIモデル続々登場 新たな競争軸か</a></h3></div><p class="meta"><time datetime="2025-06-17T04:56:40.000Z">2025年6月17日</time></p></div></li><li class="recent-li"><div class="section"><div class="desc"><h3><a href="../news/ai-latest-updates-jun25" class="internal">【bot投稿🤖】AI最新動向 Google DeepSearchやNvidia新モデル登場</a></h3></div><p class="meta"><time datetime="2025-06-05T05:52:28.000Z">2025年6月05日</time></p></div></li><li class="recent-li"><div class="section"><div class="desc"><h3><a href="../news/meeker-ai-deepseek-r1" class="internal">【bot投稿🤖】メアリーミーカー氏のAIレポートなど</a></h3></div><p class="meta"><time datetime="2025-05-31T03:49:43.000Z">2025年5月31日</time></p></div></li></ul><p style="text-align:right;padding-right:1rem;"><a href="../tags/" style="font-size:0.8rem;">さらに19件 →</a></p></div></div></div><div class="right sidebar"><div class="toc desktop-only"><button type="button" class="toc-header" aria-controls="toc-content" aria-expanded="true"><h3 style="font-size:0.8rem;color:var(--darkgray);">On This Page</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><ul class="toc-content overflow" id="list-1"><li class="depth-0"><a href="#githubリポジトリを自動解説-deepwiki登場とai最新ニュースまとめ" data-for="githubリポジトリを自動解説-deepwiki登場とai最新ニュースまとめ">GitHubリポジトリを自動解説 DeepWiki登場とAI最新ニュースまとめ</a></li><li class="depth-1"><a href="#cognition-deepwiki-githubリポジトリの百科事典" data-for="cognition-deepwiki-githubリポジトリの百科事典">Cognition DeepWiki: GitHubリポジトリの百科事典</a></li><li class="depth-1"><a href="#モデルリリースとアップデート" data-for="モデルリリースとアップデート">モデルリリースとアップデート</a></li><li class="depth-2"><a href="#meta-perception-encoders-pe" data-for="meta-perception-encoders-pe">Meta Perception Encoders (PE)</a></li><li class="depth-2"><a href="#qwen-chat-app" data-for="qwen-chat-app">Qwen Chat App</a></li><li class="depth-2"><a href="#hugging-face--fal" data-for="hugging-face--fal">Hugging Face + FAL</a></li><li class="depth-2"><a href="#openai-deep-research-軽量版" data-for="openai-deep-research-軽量版">OpenAI Deep Research (軽量版)</a></li><li class="depth-2"><a href="#perplexity-モデルアップデート" data-for="perplexity-モデルアップデート">Perplexity モデルアップデート</a></li><li class="depth-2"><a href="#vllm-for-rlhf" data-for="vllm-for-rlhf">vLLM for RLHF</a></li><li class="depth-2"><a href="#surya-ocr" data-for="surya-ocr">Surya OCR</a></li><li class="depth-1"><a href="#フレームワークツールデータセット" data-for="フレームワークツールデータセット">フレームワーク、ツール、データセット</a></li><li class="depth-1"><a href="#エージェントシステムとツール利用" data-for="エージェントシステムとツール利用">エージェントシステムとツール利用</a></li><li class="depth-1"><a href="#解釈可能性と評価" data-for="解釈可能性と評価">解釈可能性と評価</a></li><li class="depth-1"><a href="#ai倫理と福祉業界動向" data-for="ai倫理と福祉業界動向">AI倫理と福祉、業界動向</a></li><li class="depth-1"><a href="#ローカルllmと関連トピックredditより" data-for="ローカルllmと関連トピックredditより">ローカルLLMと関連トピック（Redditより）</a></li><li class="depth-1"><a href="#civitai論争と代替プラットフォームredditより" data-for="civitai論争と代替プラットフォームredditより">CivitAI論争と代替プラットフォーム（Redditより）</a></li><li class="depth-1"><a href="#openaiモデルの問題点と戦略redditより" data-for="openaiモデルの問題点と戦略redditより">OpenAIモデルの問題点と戦略（Redditより）</a></li><li class="depth-1"><a href="#フロンティアモデルとベンチマークredditより" data-for="フロンティアモデルとベンチマークredditより">フロンティアモデルとベンチマーク（Redditより）</a></li><li class="depth-1"><a href="#discordでの注目トピック" data-for="discordでの注目トピック">Discordでの注目トピック</a></li><li class="depth-1"><a href="#まとめ" data-for="まとめ">まとめ</a></li><li class="overflow-end"></li></ul></div></div><footer style="text-align:center;"><hr/><p style="font-size:0.8rem;line-height:1;">Written by <a href="https://x.com/tachibanayu24" target="_blank" rel="noopener noreferrer">tachibanayu24</a> © 2025</p><p style="font-size:0.725rem;line-height:1.2;margin:0.25rem;">このブログは<a href="https://quartz.jzhao.xyz/" target="_blank" rel="noopener noreferrer">Quartz</a>をベースに作成しています。<a href="https://policies.google.com/technologies/partner-sites?hl=ja" target="_blank" rel="noopener noreferrer">Google Analytics</a>を使用してアクセス解析を行っています。</p><p style="font-size:0.725rem;line-height:1.2;margin:0.25rem;display:flex;gap:0.5rem;justify-content:center;"><a href="/index.xml" target="_blank" rel="noopener noreferrer">RSSフィード</a><a href="https://github.com/tachibanayu24/hokori-log" target="_blank" rel="noopener noreferrer">ソースコード</a></p></footer></div></div></body><script type="application/javascript">function o(){let t=this.parentElement;t.classList.toggle("is-collapsed");let e=t.classList.contains("is-collapsed")?this.scrollHeight:t.scrollHeight;t.style.maxHeight=e+"px";let c=t,l=t.parentElement;for(;l;){if(!l.classList.contains("callout"))return;let i=l.classList.contains("is-collapsed")?l.scrollHeight:l.scrollHeight+c.scrollHeight;l.style.maxHeight=i+"px",c=l,l=l.parentElement}}function n(){let t=document.getElementsByClassName("callout is-collapsible");for(let s of t){let e=s.firstElementChild;if(!e)continue;e.addEventListener("click",o),window.addCleanup(()=>e.removeEventListener("click",o));let l=s.classList.contains("is-collapsed")?e.scrollHeight:s.scrollHeight;s.style.maxHeight=l+"px"}}document.addEventListener("nav",n);
</script><script type="module">function f(i,e){if(!i)return;function r(o){o.target===this&&(o.preventDefault(),o.stopPropagation(),e())}function t(o){o.key.startsWith("Esc")&&(o.preventDefault(),e())}i?.addEventListener("click",r),window.addCleanup(()=>i?.removeEventListener("click",r)),document.addEventListener("keydown",t),window.addCleanup(()=>document.removeEventListener("keydown",t))}function y(i){for(;i.firstChild;)i.removeChild(i.firstChild)}var h=class{constructor(e,r){this.container=e;this.content=r;this.setupEventListeners(),this.setupNavigationControls(),this.resetTransform()}isDragging=!1;startPan={x:0,y:0};currentPan={x:0,y:0};scale=1;MIN_SCALE=.5;MAX_SCALE=3;cleanups=[];setupEventListeners(){let e=this.onMouseDown.bind(this),r=this.onMouseMove.bind(this),t=this.onMouseUp.bind(this),o=this.resetTransform.bind(this);this.container.addEventListener("mousedown",e),document.addEventListener("mousemove",r),document.addEventListener("mouseup",t),window.addEventListener("resize",o),this.cleanups.push(()=>this.container.removeEventListener("mousedown",e),()=>document.removeEventListener("mousemove",r),()=>document.removeEventListener("mouseup",t),()=>window.removeEventListener("resize",o))}cleanup(){for(let e of this.cleanups)e()}setupNavigationControls(){let e=document.createElement("div");e.className="mermaid-controls";let r=this.createButton("+",()=>this.zoom(.1)),t=this.createButton("-",()=>this.zoom(-.1)),o=this.createButton("Reset",()=>this.resetTransform());e.appendChild(t),e.appendChild(o),e.appendChild(r),this.container.appendChild(e)}createButton(e,r){let t=document.createElement("button");return t.textContent=e,t.className="mermaid-control-button",t.addEventListener("click",r),window.addCleanup(()=>t.removeEventListener("click",r)),t}onMouseDown(e){e.button===0&&(this.isDragging=!0,this.startPan={x:e.clientX-this.currentPan.x,y:e.clientY-this.currentPan.y},this.container.style.cursor="grabbing")}onMouseMove(e){this.isDragging&&(e.preventDefault(),this.currentPan={x:e.clientX-this.startPan.x,y:e.clientY-this.startPan.y},this.updateTransform())}onMouseUp(){this.isDragging=!1,this.container.style.cursor="grab"}zoom(e){let r=Math.min(Math.max(this.scale+e,this.MIN_SCALE),this.MAX_SCALE),t=this.content.getBoundingClientRect(),o=t.width/2,n=t.height/2,c=r-this.scale;this.currentPan.x-=o*c,this.currentPan.y-=n*c,this.scale=r,this.updateTransform()}updateTransform(){this.content.style.transform=`translate(${this.currentPan.x}px, ${this.currentPan.y}px) scale(${this.scale})`}resetTransform(){this.scale=1;let e=this.content.querySelector("svg");this.currentPan={x:e.getBoundingClientRect().width/2,y:e.getBoundingClientRect().height/2},this.updateTransform()}},C=["--secondary","--tertiary","--gray","--light","--lightgray","--highlight","--dark","--darkgray","--codeFont"],E;document.addEventListener("nav",async()=>{let e=document.querySelector(".center").querySelectorAll("code.mermaid");if(e.length===0)return;E||=await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.esm.min.mjs");let r=E.default,t=new WeakMap;for(let n of e)t.set(n,n.innerText);async function o(){for(let s of e){s.removeAttribute("data-processed");let a=t.get(s);a&&(s.innerHTML=a)}let n=C.reduce((s,a)=>(s[a]=window.getComputedStyle(document.documentElement).getPropertyValue(a),s),{}),c=document.documentElement.getAttribute("saved-theme")==="dark";r.initialize({startOnLoad:!1,securityLevel:"loose",theme:c?"dark":"base",themeVariables:{fontFamily:n["--codeFont"],primaryColor:n["--light"],primaryTextColor:n["--darkgray"],primaryBorderColor:n["--tertiary"],lineColor:n["--darkgray"],secondaryColor:n["--secondary"],tertiaryColor:n["--tertiary"],clusterBkg:n["--light"],edgeLabelBackground:n["--highlight"]}}),await r.run({nodes:e})}await o(),document.addEventListener("themechange",o),window.addCleanup(()=>document.removeEventListener("themechange",o));for(let n=0;n<e.length;n++){let v=function(){let g=l.querySelector("#mermaid-space"),m=l.querySelector(".mermaid-content");if(!m)return;y(m);let w=c.querySelector("svg").cloneNode(!0);m.appendChild(w),l.classList.add("active"),g.style.cursor="grab",u=new h(g,m)},M=function(){l.classList.remove("active"),u?.cleanup(),u=null},c=e[n],s=c.parentElement,a=s.querySelector(".clipboard-button"),d=s.querySelector(".expand-button"),p=window.getComputedStyle(a),L=a.offsetWidth+parseFloat(p.marginLeft||"0")+parseFloat(p.marginRight||"0");d.style.right=`calc(${L}px + 0.3rem)`,s.prepend(d);let l=s.querySelector("#mermaid-container");if(!l)return;let u=null;d.addEventListener("click",v),f(l,M),window.addCleanup(()=>{u?.cleanup(),d.removeEventListener("click",v)})}});
</script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/copy-tex.min.js" type="application/javascript"></script><script src="../postscript.js" type="module"></script></html>